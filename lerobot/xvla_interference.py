# import torch
# import time
# import numpy as np
# import os
# from pathlib import Path
# import cv2

# # åŸºç¡€ç»„ä»¶
# from lerobot.policies.act.modeling_act import ACTPolicy
# from lerobot.cameras.realsense.camera_realsense import RealSenseCamera, RealSenseCameraConfig
# from src.lerobot.robots.realman.realman import Realman
# from src.lerobot.robots.realman.configuration_realman import RealmanConfig
# from Robotic_Arm.rm_robot_interface import *

# # ================= é…ç½®åŒº =================
# MODEL_PATH = "/home/robot/lerobot-main/outputs/train/checkpoints/last/pretrained_model"
# LEFT_IP, RIGHT_IP = "169.254.128.18", "169.254.128.19"
# # ç›¸æœºåºåˆ—å·
# CAMERAS = {
#     "image_top": "346522073032",
#     "image_left_wrist": "243722073715",
#     "image_right_wrist": "346522074543",
# }
# # ==========================================


# def main():
#     os.environ["CUDA_VISIBLE_DEVICES"] = "" 
    
#     # 1. åŠ è½½æ¨¡å‹
#     print(f"ğŸš€ æ­£åœ¨åŠ è½½åŸç”Ÿæ¨¡å‹: {MODEL_PATH}")
#     policy = ACTPolicy.from_pretrained(MODEL_PATH)
#     policy.eval()

#     # 2. æŒ‰ç…§å®˜æ–¹ Demo å®ä¾‹åŒ–å’Œè¿æ¥
#     arm_l = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
#     arm_r = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
    
#     handle_l = arm_l.rm_create_robot_arm(LEFT_IP, 8080)
#     handle_r = arm_r.rm_create_robot_arm(RIGHT_IP, 8080)
    
#     if handle_l.id == -1 or handle_r.id == -1:
#         print("âŒ æœºæ¢°è‡‚è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ IP")
#         return
    
#     print(f"âœ… è¿æ¥æˆåŠŸ! ID: Left({handle_l.id}), Right({handle_r.id})")

#     # å¼ºåˆ¶åˆ‡æ¢åˆ°è½¨è¿¹æ¨¡å¼ (Mode 0) ç¡®ä¿ movej ç”Ÿæ•ˆ
#     arm_l.rm_set_arm_run_mode(0)
#     arm_r.rm_set_arm_run_mode(0)

#     # 3. ç›¸æœºåˆå§‹åŒ–
#     caps = {n: cv2.VideoCapture(i) for n, i in CAMERAS.items()}

#     try:
#         print("ğŸ å¼€å§‹åŒæ­¥æ¨ç† (25Hz)...")
#         while True:
#             start_time = time.perf_counter()
            
#             # --- 4. è·å–è§‚æµ‹ (åŸç”Ÿ API è°ƒç”¨) ---
#             # ç›´æ¥è·å–è§’åº¦
#             _, joints_l = arm_l.rm_get_joint_degree()
#             _, joints_r = arm_r.rm_get_joint_degree()
            
#             # è·å–å¤¹çˆªä½ç½®
#             _, grip_l_info = arm_l.rm_get_rm_plus_state_info()
#             _, grip_r_info = arm_r.rm_get_rm_plus_state_info()
#             pos_l = grip_l_info.get('pos', [0])[0]
#             pos_r = grip_r_info.get('pos', [0])[0]
            
#             # æ„é€  26 ç»´çŠ¶æ€ (13+13 ç»“æ„)
#             s_l_7 = np.array(joints_l + [pos_l])
#             s_r_7 = np.array(joints_r + [pos_r])
#             p_zeros = torch.zeros(6)
            
#             full_state = torch.cat([
#                 torch.from_numpy(s_l_7).float(), p_zeros,
#                 torch.from_numpy(s_r_7).float(), p_zeros
#             ]).unsqueeze(0)
            
#             # æ„é€ å›¾åƒè¾“å…¥
#             batch = {"observation.state": full_state}
#             for name, cap in caps.items():
#                 ret, frame = cap.read()
#                 if ret:
#                     img = torch.from_numpy(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).float()
#                     batch[f"observation.images.{name}"] = img.permute(2, 0, 1).unsqueeze(0) / 255.0

#             # --- 5. æ¨ç† ---
#             with torch.no_grad():
#                 action = policy.select_action(batch).squeeze(0).cpu().numpy()

#             # --- 6. åŒè‡‚åŒæ­¥å¹¶è¡Œä¸‹å‘ ---
#             l_joints_cmd = action[0:6].tolist()
#             l_gripper_cmd = int(np.clip(action[6], 0, 1000))
            
#             r_joints_cmd = action[13:19].tolist()
#             r_gripper_cmd = int(np.clip(action[19], 0, 1000))

#             # ğŸš€ å…³é”®ï¼šåŒæ­¥éé˜»å¡ä¸‹å‘ (block=0)
#             # v=20 é€Ÿåº¦, r=0 è§’åº¦
#             arm_l.rm_movej(l_joints_cmd, 20, 0, 1, 0)
#             arm_r.rm_movej(r_joints_cmd, 20, 0, 1, 0)
            
#             # å¤¹çˆªä¸‹å‘
#             arm_l.rm_set_gripper_position(l_gripper_cmd, False, 1)
#             arm_r.rm_set_gripper_position(r_gripper_cmd, False, 1)

#             print(f"L_J1: {joints_l[0]:.1f}->{l_joints_cmd[0]:.1f} | R_J1: {joints_r[0]:.1f}->{r_joints_cmd[0]:.1f}")

#             # ç»´æŒé¢‘ç‡åœ¨ 25Hz å·¦å³
#             elapsed = time.perf_counter() - start_time
#             time.sleep(max(0, 0.04 - elapsed))

#     except KeyboardInterrupt:
#         print("\nğŸ›‘ åœæ­¢")
#     finally:
#         arm_l.rm_delete_robot_arm() # æŒ‰ç…§ Demo æ¸…ç†
#         arm_r.rm_delete_robot_arm()
#         for cap in caps.values(): cap.release()

# if __name__ == "__main__":
#     main()

# 

# import torch
# import time
# import numpy as np
# import os
# import cv2
# import json
# import safetensors.torch
# from pathlib import Path

# from transformers import BartTokenizerFast
# from lerobot.configs.policies import PreTrainedConfig

# try:
#     from lerobot.policies.xvla.modeling_xvla import XVLAPolicy
# except ImportError:
#     from lerobot.policies.pi0.modeling_pi0 import PI0Policy as XVLAPolicy

# from lerobot.cameras.realsense.camera_realsense import RealSenseCamera, RealSenseCameraConfig
# from Robotic_Arm.rm_robot_interface import RoboticArm, rm_thread_mode_e

# # ======================= [é…ç½®ä¸­å¿ƒ] =======================
# MODEL_PATH = "/home/robot/lerobot-main/outputs/train/008000/pretrained_model"
# LEFT_IP = "169.254.128.18"
# RIGHT_IP = "169.254.128.19"
# TASK_INSTRUCTION = "Pick the bottle to the basket_soda"

# CAMERAS_CONFIG = {
#     "image_top": "346522073032",
#     "image_left_wrist": "243722073715",
#     "image_right_wrist": "346522074543",
# }
# TARGET_KEYS = {
#     "image_top": "observation.images.image",
#     "image_right_wrist": "observation.images.image2",
#     "image_left_wrist": "observation.images.empty_camera_0"
# }

# # --- è°ƒè¯•å‚æ•° ---
# DRY_RUN = False             # âš ï¸ å®æˆ˜æ¨¡å¼
# SMOOTH_FACTOR = 0.3         # å¹³æ»‘ç³»æ•°

# # [å›ºå®šå‚æ•°]
# FORCE_RAD_TO_DEG = False
# FORCE_COMPACT_INDEX = False
# # ========================================================

# def load_stats(model_path, device):
#     print("ğŸ” æ­£åœ¨åŠ è½½åå½’ä¸€åŒ–æ•°æ®...")
#     stats_file_name = "policy_postprocessor_step_0_unnormalizer_processor.safetensors"
#     stats_path = os.path.join(model_path, stats_file_name)
#     if not os.path.exists(stats_path):
#         stats_path = os.path.join(os.path.dirname(model_path.rstrip("/")), stats_file_name)

#     if os.path.exists(stats_path):
#         try:
#             tensors = safetensors.torch.load_file(stats_path)
#             mean, scale = None, None
#             for key in tensors.keys():
#                 if "action" in key and "mean" in key: mean = tensors[key]
#                 if "action" in key and ("scale" in key or "std" in key): scale = tensors[key]

#             if mean is not None:
#                 mean = mean.to(device=device, dtype=torch.float32)
#                 scale = scale.to(device=device, dtype=torch.float32)
#                 print(f"âœ… ç»Ÿè®¡æ•°æ®åŠ è½½æˆåŠŸ | Mean Shape: {mean.shape}")
#                 return mean, scale
#         except Exception as e:
#             print(f"âŒ è¯»å–å¤±è´¥: {e}")

#     return torch.tensor(0.0, device=device), torch.tensor(1.0, device=device)

# # [ä¿ç•™] ä¸­å¿ƒè£å‰ªå‡½æ•°
# def center_crop_and_resize(img, target_size=224):
#     h, w, _ = img.shape
#     min_dim = min(h, w)
#     top = (h - min_dim) // 2
#     left = (w - min_dim) // 2
#     img_cropped = img[top:top+min_dim, left:left+min_dim]
#     img_resized = cv2.resize(img_cropped, (target_size, target_size))
#     return img_resized

# def main():
#     try:
#         if torch.cuda.is_available():
#             device = torch.device("cuda")
#             print(f"ğŸš€ GPU: {torch.cuda.get_device_name(0)}")
#         else:
#             raise Exception
#     except:
#         device = torch.device("cpu")

#     # 1. åŠ è½½ç»Ÿè®¡æ•°æ®
#     action_mean, action_std = load_stats(MODEL_PATH, device)

#     # 2. åŠ è½½æ¨¡å‹
#     print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹...")
#     try:
#         config = PreTrainedConfig.from_pretrained(MODEL_PATH)
#         config.max_len_seq = 2048
#         config.dtype = "float32"
#         policy = XVLAPolicy(config)

#         model_file = os.path.join(MODEL_PATH, "model.safetensors")
#         state_dict = safetensors.torch.load_file(model_file)

#         pos_emb_key = "model.transformer.pos_emb"
#         if pos_emb_key in state_dict:
#             old_emb = state_dict[pos_emb_key]
#             new_emb_placeholder = policy.model.transformer.pos_emb.data.clone()
#             new_emb_placeholder[:, :old_emb.shape[1], :] = old_emb
#             state_dict[pos_emb_key] = new_emb_placeholder

#         enc_k = "model.vlm.language_model.model.encoder.embed_tokens.weight"
#         shared_k = "model.vlm.language_model.model.shared.weight"
#         if enc_k in state_dict and shared_k not in state_dict:
#             state_dict[shared_k] = state_dict[enc_k]

#         policy.load_state_dict(state_dict, strict=False)
#         policy.to(dtype=torch.float32, device=device)
#         policy.eval()
#         print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
#     except Exception as e:
#         print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
#         return

#     # 3. åˆ†è¯å™¨
#     try:
#         tokenizer = BartTokenizerFast.from_pretrained(MODEL_PATH, local_files_only=True)
#         text_tokens = tokenizer(TASK_INSTRUCTION, return_tensors="pt", max_length=policy.config.tokenizer_max_length, truncation=True, padding="max_length")["input_ids"].to(device)
#     except:
#         return

#     # 4. ç¡¬ä»¶åˆå§‹åŒ–
#     arm_l = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
#     arm_r = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
#     handle_l = arm_l.rm_create_robot_arm(LEFT_IP, 8080)
#     handle_r = arm_r.rm_create_robot_arm(RIGHT_IP, 8080)

#     if handle_l.id == -1 or handle_r.id == -1:
#         print("âŒ æœºæ¢°è‡‚è¿æ¥å¤±è´¥")
#         return

#     caps = {}
#     for name, sn in CAMERAS_CONFIG.items():
#         try:
#             cfg = RealSenseCameraConfig(serial_number_or_name=sn, fps=30, width=424, height=240)
#             caps[name] = RealSenseCamera(cfg)
#             caps[name].connect()
#             print(f"ğŸ“¸ {name} å°±ç»ª")
#         except: pass

#     # 5. å˜é‡åˆå§‹åŒ–
#     _, curr_l = arm_l.rm_get_joint_degree()
#     _, curr_r = arm_r.rm_get_joint_degree()
#     l_cmd_smooth = np.array(curr_l, dtype=np.float32)
#     r_cmd_smooth = np.array(curr_r, dtype=np.float32)

#     last_l_grip_cmd = -1
#     last_r_grip_cmd = -1

#     # [å…³é”®ä¿®å¤] ImageNet æ ‡å‡†åŒ–å‚æ•° (PyTorch æ ‡å‡†)
#     IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)
#     IMAGENET_STD = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)

#     print(f"\nğŸ ç³»ç»Ÿå¯åŠ¨ | DRY_RUN={DRY_RUN} | Vision=RGB+CenterCrop+Norm")
#     time.sleep(1)

#     try:
#         while True:
#             loop_start = time.perf_counter()

#             # --- State ---
#             _, joints_l = arm_l.rm_get_joint_degree()
#             _, joints_r = arm_r.rm_get_joint_degree()

#             pad_6 = np.zeros(6)
#             state_data = np.concatenate([
#                 np.array(joints_l), [0],  # Index 0-6
#                 pad_6,                    # Index 7-12
#                 np.array(joints_r), [0],  # Index 13-19
#                 pad_6                     # Index 20-25
#             ])

#             full_state = torch.from_numpy(state_data).float().unsqueeze(0).to(device)
#             batch = { "observation.state": full_state, "observation.language.tokens": text_tokens }

#             vis_frames = []

#             # --- è§†è§‰å¤„ç†æ ¸å¿ƒå¾ªç¯ ---
#             for name, cap in caps.items():
#                 frame = cap.read() # OpenCV è¿”å› BGR
#                 if frame is not None:
#                     # 1. [å…³é”®] ä¸­å¿ƒè£å‰ª (ä¿æŒç‰©ä½“æ¯”ä¾‹)
#                     img_cropped = center_crop_and_resize(frame, 224)

#                     # 2. [ç»™äººç±»] å¯è§†åŒ– (BGR)
#                     vis_frames.append(img_cropped)

#                     # 3. [å…³é”®] BGR è½¬ RGB (ç»™æ¨¡å‹)
#                     img_rgb = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2RGB)

#                     # 4. [å…³é”®] è½¬ Tensor å¹¶ ImageNet æ ‡å‡†åŒ–
#                     img_tensor = torch.from_numpy(img_rgb).float().permute(2,0,1).unsqueeze(0).to(device) / 255.0
#                     img_tensor = (img_tensor - IMAGENET_MEAN) / IMAGENET_STD

#                     batch[TARGET_KEYS[name]] = img_tensor
#                 else:
#                     batch[TARGET_KEYS[name]] = torch.zeros((1,3,224,224)).to(device)
#                     vis_frames.append(np.zeros((224, 224, 3), dtype=np.uint8))

#             # --- æ˜¾ç¤ºå¯è§†åŒ–çª—å£ (æ£€æŸ¥é¢œè‰²å’Œæ¯”ä¾‹) ---
#             if len(vis_frames) > 0:
#                 # è¿™é‡Œæ˜¾ç¤ºçš„æ˜¯ BGRï¼Œæ‰€ä»¥æ©˜è‰²åº”è¯¥æ˜¯æ©˜è‰²
#                 cv2.imshow("Corrected View (BGR for Humans)", np.hstack(vis_frames))
#                 if cv2.waitKey(1) & 0xFF == ord('q'): break

#             # --- æ¨ç† ---
#             try:
#                 with torch.no_grad():
#                     raw_action = policy.select_action(batch).squeeze(0)
#                     if raw_action.shape[0] != action_mean.shape[0]:
#                         curr_mean = action_mean[:raw_action.shape[0]]
#                         curr_std = action_std[:raw_action.shape[0]]
#                     else:
#                         curr_mean = action_mean
#                         curr_std = action_std

#                     real_action = raw_action * curr_std + curr_mean
#                     output_action = real_action.float().cpu().numpy()
#             except Exception as e:
#                 print(f"âŒ æ¨ç†å´©æºƒ: {e}")
#                 break

#             # --- è§£æ ---
#             target_l = output_action[0:6]
#             l_grip_raw = output_action[6]
#             target_r = output_action[13:19]
#             r_grip_raw = output_action[19]

#             l_cmd_smooth = l_cmd_smooth * (1 - SMOOTH_FACTOR) + target_l * SMOOTH_FACTOR
#             r_cmd_smooth = r_cmd_smooth * (1 - SMOOTH_FACTOR) + target_r * SMOOTH_FACTOR

#             # [å…³é”®] å¤¹çˆªæ”¾å¤§ç³»æ•°è°ƒæ•´ï¼šä» *100 æ”¹ä¸º *10
#             # å‡è®¾ Raw è¾“å‡ºåœ¨ 0-100 ä¹‹é—´ï¼Œä¹˜ä»¥ 10 æ˜ å°„åˆ° 0-1000
#             l_grip_pos = int(np.clip(l_grip_raw * 10, 0, 1000))
#             r_grip_pos = int(np.clip(r_grip_raw * 10, 0, 1000))

#             # --- æ‰“å° ---
#             # fps = 1.0 / (time.perf_counter() - loop_start)
#             fps = 30.0
#             print(f"\rFPS: {fps:.1f} | L_Grip: {l_grip_pos:<4} | R_Grip: {r_grip_pos:<4} | R_J1: {r_cmd_smooth[0]:.1f}", end="")

#             # --- æ‰§è¡Œ ---
#             if not DRY_RUN:
#                 arm_l.rm_movej_canfd(l_cmd_smooth.tolist(), False, 0, 0, 0)
#                 arm_r.rm_movej_canfd(r_cmd_smooth.tolist(), False, 0, 0, 0)

#                 if abs(l_grip_pos - last_l_grip_cmd) > 20:
#                     arm_l.rm_set_gripper_position(l_grip_pos, False, 1)
#                     last_l_grip_cmd = l_grip_pos

#                 if abs(r_grip_pos - last_r_grip_cmd) > 20:
#                     arm_r.rm_set_gripper_position(r_grip_pos, False, 1)
#                     last_r_grip_cmd = r_grip_pos
#             else:
#                 time.sleep(0.05)

#     except KeyboardInterrupt:
#         print("\nğŸ‘‹ åœæ­¢è¿è¡Œ")
#     finally:
#         arm_l.rm_delete_robot_arm()
#         arm_r.rm_delete_robot_arm()
#         for cap in caps.values():
#             cap.disconnect()
#         cv2.destroyAllWindows()

# if __name__ == "__main__":
#     main()

import torch
import time
import numpy as np
import os
import cv2
import safetensors.torch
from dataclasses import dataclass
from typing import Dict, Optional, Tuple

from transformers import BartTokenizerFast
from lerobot.configs.policies import PreTrainedConfig
try:
    from lerobot.policies.xvla.modeling_xvla import XVLAPolicy
except ImportError:
    from lerobot.policies.pi0.modeling_pi0 import PI0Policy as XVLAPolicy

from lerobot.cameras.realsense.camera_realsense import RealSenseCamera, RealSenseCameraConfig
from Robotic_Arm.rm_robot_interface import RoboticArm, rm_thread_mode_e

# ================= é…ç½®ç±» =================
@dataclass
class InferenceConfig:
    # è·¯å¾„é…ç½®
    model_path: str = "/home/robot/lerobot-main/outputs/train/008000/pretrained_model"
    
    # ç¡¬ä»¶é…ç½®
    left_ip: str = "169.254.128.18"
    right_ip: str = "169.254.128.19"
    cameras_config: Dict[str, str] = None
    
    # ä»»åŠ¡é…ç½®
    task_instruction: str = "Pick the bottle to the basket_soda"
    
    # å¹³æ»‘ç³»æ•°
    smooth_factor: float = 0.3
    dry_run: bool = True
    loop_rate: float = 30.0
    
    def __post_init__(self):
        if self.cameras_config is None:
            self.cameras_config = {
                "image_top": "346522073032",
                "image_left_wrist": "243722073715",
                "image_right_wrist": "346522074543",
            }

# ================= æ¨ç†æ ¸å¿ƒç±» =================
class Inference:
    def __init__(self, cfg: InferenceConfig):
        self.cfg = cfg
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸš€ è®¾å¤‡: {self.device}")
        
        # 1. åŠ è½½ç»Ÿè®¡æ•°æ®
        self.action_mean, self.action_std = self._load_output_stats()
        self.state_mean, self.state_std = self._load_input_stats()

        # 2. åŠ è½½æ¨¡å‹
        self.policy = self._load_model()
        self.text_tokens = self._load_tokenizer()
        
        # 3. åˆå§‹åŒ–ç¡¬ä»¶
        self.arm_l, self.arm_r = self._init_robots()
        self.caps = self._init_cameras()
        self._init_runtime_state()
        
        # å›¾åƒå½’ä¸€åŒ–æ ‡å‡† (0.5)
        self.IMG_MEAN = torch.tensor([0.5, 0.5, 0.5], device=self.device).view(1, 3, 1, 1)
        self.IMG_STD = torch.tensor([0.5, 0.5, 0.5], device=self.device).view(1, 3, 1, 1)
        
        self.target_keys = {
            "image_top": "observation.images.image",
            "image_right_wrist": "observation.images.image2",
            "image_left_wrist": "observation.images.empty_camera_0"
        }

    def _load_output_stats(self):
        print("ğŸ” åŠ è½½è¾“å‡ºç»Ÿè®¡ (Action)...")
        stats_file = "policy_postprocessor_step_0_unnormalizer_processor.safetensors"
        paths = [
            os.path.join(self.cfg.model_path, stats_file),
            os.path.join(os.path.dirname(self.cfg.model_path.rstrip("/")), stats_file)
        ]
        
        for path in paths:
            if os.path.exists(path):
                try:
                    tensors = safetensors.torch.load_file(path)
                    mean, scale = None, None
                    # æš´åŠ›æŸ¥æ‰¾ Key
                    for key in tensors.keys():
                        if "action" in key and "mean" in key: mean = tensors[key]
                        if "action" in key and ("scale" in key or "std" in key): scale = tensors[key]
                    
                    if mean is not None:
                        mean = mean.to(self.device, dtype=torch.float32)
                        scale = scale.to(self.device, dtype=torch.float32)
                        # ç¡®ä¿æ˜¯ [1, 26]
                        if mean.ndim == 1: mean = mean.unsqueeze(0)
                        if scale.ndim == 1: scale = scale.unsqueeze(0)
                        print(f"âœ… è¾“å‡ºç»Ÿè®¡åŠ è½½æˆåŠŸ | Shape: {mean.shape}")
                        return mean, scale
                except Exception as e:
                    print(f"âŒ è¾“å‡ºç»Ÿè®¡è¯»å–å¤±è´¥: {e}")
        
        print("âš ï¸ æœªæ‰¾åˆ°è¾“å‡ºç»Ÿè®¡ï¼ä½¿ç”¨é»˜è®¤å€¼")
        return torch.tensor(0.0, device=self.device), torch.tensor(1.0, device=self.device)

    def _load_input_stats(self):
        print("ğŸ” åŠ è½½è¾“å…¥ç»Ÿè®¡ (State)...")
        stats_file = "policy_preprocessor_step_7_normalizer_processor.safetensors"
        paths = [
            os.path.join(self.cfg.model_path, stats_file),
            os.path.join(os.path.dirname(self.cfg.model_path.rstrip("/")), stats_file)
        ]
        for path in paths:
            if os.path.exists(path):
                try:
                    tensors = safetensors.torch.load_file(path)
                    mean, scale = None, None
                    for key in tensors.keys():
                        if "observation.state" in key and "mean" in key: mean = tensors[key]
                        if "observation.state" in key and ("scale" in key or "std" in key): scale = tensors[key]

                    if mean is not None:
                        mean = mean.to(self.device, dtype=torch.float32)
                        scale = scale.to(self.device, dtype=torch.float32)
                        if mean.ndim == 1: mean = mean.unsqueeze(0)
                        if scale.ndim == 1: scale = scale.unsqueeze(0)
                        print(f"âœ… è¾“å…¥ç»Ÿè®¡åŠ è½½æˆåŠŸ | Mean: {mean[0,:6].cpu().numpy()}")
                        return mean, scale
                except Exception as e: print(f"âŒ å¤±è´¥: {e}")
        print("âš ï¸ æœªæ‰¾åˆ°è¾“å…¥ç»Ÿè®¡ï¼")
        return torch.tensor(0.0, device=self.device), torch.tensor(1.0, device=self.device)

    def _load_model(self):
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹...")
        config = PreTrainedConfig.from_pretrained(self.cfg.model_path)
        config.max_len_seq = 2048
        config.dtype = "float32"
        policy = XVLAPolicy(config)
        state_dict = safetensors.torch.load_file(os.path.join(self.cfg.model_path, "model.safetensors"))
        self._patch_state_dict(policy, state_dict)
        policy.load_state_dict(state_dict, strict=False)
        policy.to(dtype=torch.float32, device=self.device).eval()
        return policy

    def _patch_state_dict(self, policy, state_dict):
        pos_key = "model.transformer.pos_emb"
        if pos_key in state_dict:
            old = state_dict[pos_key]
            new = policy.model.transformer.pos_emb.data.clone()
            if old.shape != new.shape:
                new[:, :old.shape[1], :] = old
                state_dict[pos_key] = new
        enc_k = "model.vlm.language_model.model.encoder.embed_tokens.weight"
        shared_k = "model.vlm.language_model.model.shared.weight"
        if enc_k in state_dict and shared_k not in state_dict: state_dict[shared_k] = state_dict[enc_k]

    def _load_tokenizer(self):
        try:
            tokenizer = BartTokenizerFast.from_pretrained(self.cfg.model_path, local_files_only=True)
            return tokenizer(self.cfg.task_instruction, return_tensors="pt", 
                max_length=self.policy.config.tokenizer_max_length, 
                truncation=True, padding="max_length")["input_ids"].to(self.device)
        except: return None

    def _init_robots(self):
        arm_l = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
        arm_r = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
        hl = arm_l.rm_create_robot_arm(self.cfg.left_ip, 8080)
        hr = arm_r.rm_create_robot_arm(self.cfg.right_ip, 8080)
        if hl.id == -1 or hr.id == -1: raise RuntimeError("âŒ è¿æ¥å¤±è´¥")
        return arm_l, arm_r

    def _init_cameras(self):
        caps = {}
        for name, sn in self.cfg.cameras_config.items():
            try:
                cfg = RealSenseCameraConfig(serial_number_or_name=sn, fps=30, width=640, height=480)
                cam = RealSenseCamera(cfg)
                cam.connect()
                caps[name] = cam
                print(f"ğŸ“¸ {name} å°±ç»ª")
            except: pass
        return caps

    def _init_runtime_state(self):
        _, curr_l = self.arm_l.rm_get_joint_degree()
        _, curr_r = self.arm_r.rm_get_joint_degree()
        self.cmd_smooth_l = np.array(curr_l, dtype=np.float32)
        self.cmd_smooth_r = np.array(curr_r, dtype=np.float32)
        self.last_grip_l = -1
        self.last_grip_r = -1

    def _process_image(self, img_bgr):
        h, w, _ = img_bgr.shape
        min_dim = min(h, w)
        top = (h - min_dim) // 2
        left = (w - min_dim) // 2
        img = img_bgr[top:top+min_dim, left:left+min_dim]
        img = cv2.resize(img, (224, 224))
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        tensor = torch.from_numpy(img_rgb).float().permute(2,0,1).unsqueeze(0).to(self.device) / 255.0
        tensor = (tensor - self.IMG_MEAN) / self.IMG_STD
        return tensor, img

    def get_observation(self):
        _, jl = self.arm_l.rm_get_joint_degree()
        _, jr = self.arm_r.rm_get_joint_degree()
        
        jl_np = np.array(jl)
        jr_np = np.array(jr)
        pad = np.zeros(6)
        state = np.concatenate([jl_np, [0], pad, jr_np, [0], pad])
        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)
        
        # å½’ä¸€åŒ–
        normalized_state = (state_tensor - self.state_mean) / self.state_std
        
        batch = {
            "observation.state": normalized_state,
            "observation.language.tokens": self.text_tokens
        }
        
        vis_frames = []
        for name, cap in self.caps.items():
            frame = cap.read()
            if frame is not None:
                tensor, vis_img = self._process_image(frame)
                batch[self.target_keys[name]] = tensor
                vis_frames.append(vis_img)
            else:
                batch[self.target_keys[name]] = torch.zeros((1,3,224,224), device=self.device)
                vis_frames.append(np.zeros((224,224,3), dtype=np.uint8))
        return batch, vis_frames

    def execute_action(self, action):
        target_l = action[0:6]
        target_r = action[13:19]
        grip_l = int(np.clip(action[6] * 10, 0, 1000))
        grip_r = int(np.clip(action[19] * 10, 0, 1000))
        
        self.cmd_smooth_l = self.cmd_smooth_l * (1 - self.cfg.smooth_factor) + target_l * self.cfg.smooth_factor
        self.cmd_smooth_r = self.cmd_smooth_r * (1 - self.cfg.smooth_factor) + target_r * self.cfg.smooth_factor
        
        print(f"\rL_Grip: {grip_l:<4} | R_Grip: {grip_r:<4} | R_J1: {self.cmd_smooth_r[0]:.1f}", end="")
        
        if not self.cfg.dry_run:
            self.arm_l.rm_movej_canfd(self.cmd_smooth_l.tolist(), False, 0, 0, 0)
            self.arm_r.rm_movej_canfd(self.cmd_smooth_r.tolist(), False, 0, 0, 0)
            if abs(grip_l - self.last_grip_l) > 20:
                self.arm_l.rm_set_gripper_position(grip_l, False, 1)
                self.last_grip_l = grip_l
            if abs(grip_r - self.last_grip_r) > 20:
                self.arm_r.rm_set_gripper_position(grip_r, False, 1)
                self.last_grip_r = grip_r

    def run(self):
        print(f"\nğŸ å¼€å§‹æ¨ç†å¾ªç¯ | Rate: {self.cfg.loop_rate}Hz")
        while True:
            loop_start = time.perf_counter()
            batch, vis_frames = self.get_observation()
            
            if vis_frames:
                cv2.imshow("Robot View", np.hstack(vis_frames))
                if cv2.waitKey(1) & 0xFF == ord('q'): break
            
            with torch.no_grad():
                raw_action = self.policy.select_action(batch).squeeze(0)
                
                # ----------------------------------------------------
                # ğŸ›¡ï¸ è°ƒè¯• & æš´åŠ›ä¿®å¤åŒº
                # ----------------------------------------------------
                # å¦‚æœè¿™ä¸€è¡Œæ‰“å°å‡ºæ¥äº†ï¼Œè¯´æ˜ä»£ç æ›´æ–°æˆåŠŸäº†ï¼
                # print(f"DEBUG SHAPE: Action={raw_action.shape} | Stats={self.action_mean.shape}")

                # æš´åŠ›ä¿®æ­£ï¼šåªè¦æœ€åä¸€ä¸ªç»´åº¦ä¸æ˜¯ 26ï¼Œå°±ä¸€å®šæ˜¯ç»´åº¦åäº†ï¼Œç«‹åˆ»è½¬ç½®ï¼
                # é’ˆå¯¹ [26, 20] è¿™ç§æƒ…å†µ
                if raw_action.ndim > 1 and raw_action.shape[-1] != 26:
                    raw_action = raw_action.t()
                
                # é’ˆå¯¹å¯èƒ½çš„ [26] (1D) æƒ…å†µï¼Œå¼ºåˆ¶å‡ç»´åˆ° [1, 26] ä»¥é˜²ä¸‡ä¸€
                if raw_action.ndim == 1 and raw_action.shape[0] == 26:
                    raw_action = raw_action.unsqueeze(0)

                # ----------------------------------------------------
                
                curr_mean = self.action_mean
                curr_std = self.action_std
                
                real_action = raw_action * curr_std + curr_mean
                action_numpy = real_action.float().cpu().numpy()
            
            # å–ç¬¬ä¸€å¸§æ‰§è¡Œ
            # å¦‚æœ raw_action å˜æˆäº† [20, 26]ï¼Œå– [0] å°±æ˜¯ç¬¬ä¸€å¸§ [26]
            self.execute_action(action_numpy[0])
            
            time.sleep(max(0, (1.0 / self.cfg.loop_rate) - (time.perf_counter() - loop_start)))

    def cleanup(self):
        print("\nğŸ§¹ æ¸…ç†...")
        self.arm_l.rm_delete_robot_arm()
        self.arm_r.rm_delete_robot_arm()
        for cap in self.caps.values(): cap.disconnect()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    config = InferenceConfig(task_instruction="Pick the bottle to the basket_soda")
    inference = Inference(config)
    inference.run()