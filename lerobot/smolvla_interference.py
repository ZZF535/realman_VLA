import torch
import time
import numpy as np
import os
import cv2
import json
import safetensors.torch
from pathlib import Path

from transformers import BartTokenizerFast, AutoTokenizer
from lerobot.configs.policies import PreTrainedConfig

# ======================= [ÈÖçÁΩÆ‰∏≠ÂøÉ] =======================
MODEL_PATH = "/home/robot/lerobot-main/outputs/train/020000/pretrained_model"
LEFT_IP = "169.254.128.18"
RIGHT_IP = "169.254.128.19"
TASK_INSTRUCTION = "Êää...ÊîæËøõ..._ÊâìÂºÄÊ¥óË°£Êú∫ÂèñÂá∫Ë°£Êúç_ÊääË°£ÊúçÊîæÂà∞Ê¥óË°£Êú∫Èáå"

CAMERAS_CONFIG = {
    "image_top": "346522073032",
    "image_left_wrist": "243722073715",
    "image_right_wrist": "346522074543",
}
TARGET_KEYS = {
    "image_top": "observation.images.image",
    "image_right_wrist": "observation.images.image2",
    "image_left_wrist": "observation.images.empty_camera_0"
}

# --- Ë∞ÉËØïÂèÇÊï∞ ---
DRY_RUN = True             
SMOOTH_FACTOR = 0.3         
TARGET_IMAGE_SIZE = 384     # SigLIP ÈªòËÆ§ 384

FORCE_RAD_TO_DEG = False     
FORCE_COMPACT_INDEX = False  
# ========================================================

# --- Âä®ÊÄÅÂØºÂÖ•Á≠ñÁï•Á±ª ---
# ‰∏∫‰∫ÜÈò≤Ê≠¢ LeRobot ÁâàÊú¨Â∑ÆÂºÇÔºåÊàë‰ª¨Â∞ùËØïÂØºÂÖ•ÊâÄÊúâÂèØËÉΩÁöÑÁ±ª
try:
    from lerobot.policies.factory import make_policy
    USE_FACTORY = True
except ImportError:
    try:
        from lerobot.policies.factory import make_policy
        USE_FACTORY = True
    except ImportError:
        USE_FACTORY = False

# Â∞ùËØïÂØºÂÖ•ÂÖ∑‰ΩìÁöÑÁ≠ñÁï•Á±ª‰Ωú‰∏∫Â§áÈÄâ
PolicyClasses = {}
try:
    from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy
    PolicyClasses['SmolVLAConfig'] = SmolVLAPolicy
except ImportError:
    pass

try:
    from lerobot.policies.xvla.modeling_xvla import XVLAPolicy
    PolicyClasses['XVLAConfig'] = XVLAPolicy
except ImportError:
    pass

from lerobot.cameras.realsense.camera_realsense import RealSenseCamera, RealSenseCameraConfig
from Robotic_Arm.rm_robot_interface import RoboticArm, rm_thread_mode_e


def load_stats(model_path, device):
    print("üîç Ê≠£Âú®Âä†ËΩΩÂèçÂΩí‰∏ÄÂåñÊï∞ÊçÆ...")
    stats_file_name = "policy_postprocessor_step_0_unnormalizer_processor.safetensors"
    stats_path = os.path.join(model_path, stats_file_name)
    if not os.path.exists(stats_path):
        stats_path = os.path.join(os.path.dirname(model_path.rstrip("/")), stats_file_name)

    if os.path.exists(stats_path):
        try:
            tensors = safetensors.torch.load_file(stats_path)
            mean, scale = None, None
            for key in tensors.keys():
                if "action" in key and "mean" in key: mean = tensors[key]
                if "action" in key and ("scale" in key or "std" in key): scale = tensors[key]

            if mean is not None:
                mean = mean.to(device=device, dtype=torch.float32)
                scale = scale.to(device=device, dtype=torch.float32)
                print(f"‚úÖ ÁªüËÆ°Êï∞ÊçÆÂä†ËΩΩÊàêÂäü | Mean Shape: {mean.shape}")
                return mean, scale
        except Exception as e:
            print(f"‚ùå ËØªÂèñÂ§±Ë¥•: {e}")
    
    return torch.tensor(0.0, device=device), torch.tensor(1.0, device=device)

def center_crop_and_resize(img, target_size):
    h, w, _ = img.shape
    min_dim = min(h, w)
    top = (h - min_dim) // 2
    left = (w - min_dim) // 2
    img_cropped = img[top:top+min_dim, left:left+min_dim]
    img_resized = cv2.resize(img_cropped, (target_size, target_size))
    return img_resized

def main():
    try:
        if torch.cuda.is_available():
            device = torch.device("cuda")
            print(f"üöÄ GPU: {torch.cuda.get_device_name(0)}")
        else:
            raise Exception
    except:
        device = torch.device("cpu")

    action_mean, action_std = load_stats(MODEL_PATH, device)

    print(f"üì¶ Ê≠£Âú®Âä†ËΩΩ SmolVLA Ê®°Âûã...")
    policy = None
    
    # --- 1. ‰ºòÂÖàÂ∞ùËØï Factory Âä†ËΩΩ (Ëá™Âä®ËØÜÂà´Á±ª) ---
    if USE_FACTORY:
        try:
            # [‰øÆÊ≠£] Áõ¥Êé•‰º†Ë∑ØÂæÑ‰Ωú‰∏∫‰ΩçÁΩÆÂèÇÊï∞ÔºåÂÖºÂÆπÊÄßÊúÄÂ•Ω
            policy = make_policy(MODEL_PATH)
            policy.to(device)
            policy.eval()
            print("‚úÖ ÈÄöËøá Factory Âä†ËΩΩÊàêÂäü")
        except Exception as e:
            print(f"‚ö†Ô∏è Factory Âä†ËΩΩÂ§±Ë¥• ({e})ÔºåÂ∞ùËØïÊâãÂä®Âä†ËΩΩ...")
            policy = None

    # --- 2. ÊâãÂä®Âä†ËΩΩÂõûÈÄÄ ---
    if policy is None:
        try:
            config = PreTrainedConfig.from_pretrained(MODEL_PATH)
            config_class_name = config.__class__.__name__
            print(f"‚ÑπÔ∏è ËØÜÂà´Âà∞ÈÖçÁΩÆÁ±ª: {config_class_name}")

            # Ê†πÊçÆ Config Á±ªÂûãÈÄâÊã©Ê≠£Á°ÆÁöÑ Policy Á±ª
            if config_class_name in PolicyClasses:
                PolicyClass = PolicyClasses[config_class_name]
                print(f"‚ÑπÔ∏è ‰ΩøÁî®Á≠ñÁï•Á±ª: {PolicyClass.__name__}")
            else:
                # ÊúÄÂêéÁöÑÂ∞ùËØïÔºöÂ¶ÇÊûúÊâæ‰∏çÂà∞ÂØπÂ∫îÁöÑÁ±ªÔºå‰∏îÂêçÂ≠óÈáåÊúâ SmolÔºåÂ∞ùËØïÂä®ÊÄÅÂØºÂÖ•
                if 'Smol' in config_class_name:
                     from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy as PolicyClass
                else:
                     from lerobot.policies.xvla.modeling_xvla import XVLAPolicy as PolicyClass
            
            # Êõ¥Êñ∞ÂàÜËæ®Áéá
            global TARGET_IMAGE_SIZE
            if hasattr(config, "image_size") and config.image_size:
                TARGET_IMAGE_SIZE = config.image_size
                print(f"‚ÑπÔ∏è Config Image Size: {TARGET_IMAGE_SIZE}")

            policy = PolicyClass(config)
            
            model_file = os.path.join(MODEL_PATH, "model.safetensors")
            state_dict = safetensors.torch.load_file(model_file)
            
            # ÊùÉÈáç‰øÆÂ§çÈÄªËæë
            if "model.transformer.pos_emb" in state_dict:
                old_emb = state_dict["model.transformer.pos_emb"]
                new_emb = policy.model.transformer.pos_emb.data.clone()
                new_emb[:, :old_emb.shape[1], :] = old_emb
                state_dict["model.transformer.pos_emb"] = new_emb
            
            policy.load_state_dict(state_dict, strict=False)
            policy.to(dtype=torch.float32, device=device)
            policy.eval()
            print("‚úÖ ÊâãÂä®Âä†ËΩΩÊàêÂäü")
        except Exception as e:
            print(f"‚ùå Ëá¥ÂëΩÈîôËØØ: Ê®°ÂûãÂä†ËΩΩÂÆåÂÖ®Â§±Ë¥•: {e}")
            return

    # Âä†ËΩΩ Tokenizer
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True)
        max_len = getattr(policy.config, "tokenizer_max_length", 128) 
        text_tokens = tokenizer(TASK_INSTRUCTION, return_tensors="pt", max_length=max_len, truncation=True, padding="max_length")["input_ids"].to(device)
    except:
        print("‚ùå ÂàÜËØçÂô®Âä†ËΩΩÂ§±Ë¥•")
        return

    # Á°¨‰ª∂ËøûÊé•
    arm_l = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
    arm_r = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
    handle_l = arm_l.rm_create_robot_arm(LEFT_IP, 8080)
    handle_r = arm_r.rm_create_robot_arm(RIGHT_IP, 8080)
    
    if handle_l.id == -1 or handle_r.id == -1:
        print("‚ùå Êú∫Ê¢∞ËáÇËøûÊé•Â§±Ë¥•")
        return

    caps = {}
    for name, sn in CAMERAS_CONFIG.items():
        try:
            cfg = RealSenseCameraConfig(serial_number_or_name=sn, fps=30, width=640, height=480)
            caps[name] = RealSenseCamera(cfg)
            caps[name].connect()
            print(f"üì∏ {name} Â∞±Áª™")
        except: pass

    _, curr_l = arm_l.rm_get_joint_degree()
    _, curr_r = arm_r.rm_get_joint_degree()
    l_cmd_smooth = np.array(curr_l, dtype=np.float32)
    r_cmd_smooth = np.array(curr_r, dtype=np.float32)
    
    last_l_grip_cmd = -1 
    last_r_grip_cmd = -1

    # SmolVLA/SigLIP ÂΩí‰∏ÄÂåñÂèÇÊï∞ (Mean=0.5, Std=0.5)
    SIGLIP_MEAN = torch.tensor([0.5, 0.5, 0.5], device=device).view(1, 3, 1, 1)
    SIGLIP_STD = torch.tensor([0.5, 0.5, 0.5], device=device).view(1, 3, 1, 1)

    print(f"\nüèÅ Á≥ªÁªüÂêØÂä® | SMOOTH={SMOOTH_FACTOR} | Size={TARGET_IMAGE_SIZE}")
    time.sleep(1)

    try:
        while True:
            loop_start = time.perf_counter()
            
            _, joints_l = arm_l.rm_get_joint_degree() 
            _, joints_r = arm_r.rm_get_joint_degree()
            
            # State
            pad_6 = np.zeros(6)
            state_data = np.concatenate([
                np.array(joints_l), [0],  # 0-6
                pad_6,                    # 7-12
                np.array(joints_r), [0],  # 13-19
                pad_6                     # 20-25
            ])

            full_state = torch.from_numpy(state_data).float().unsqueeze(0).to(device)
            batch = { "observation.state": full_state, "observation.language.tokens": text_tokens }
            
            vis_frames = []
            for name, cap in caps.items():
                frame = cap.read()
                if frame is not None:
                    # 1. Center Crop + Resize
                    img = center_crop_and_resize(frame, TARGET_IMAGE_SIZE)
                    vis_frames.append(img)
                    
                    # 2. BGR -> RGB
                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    
                    # 3. Norm (SigLIP style)
                    img_tensor = torch.from_numpy(img_rgb).float().permute(2,0,1).unsqueeze(0).to(device) / 255.0
                    img_tensor = (img_tensor - SIGLIP_MEAN) / SIGLIP_STD
                    
                    batch[TARGET_KEYS[name]] = img_tensor
                else:
                    batch[TARGET_KEYS[name]] = torch.zeros((1,3,TARGET_IMAGE_SIZE,TARGET_IMAGE_SIZE)).to(device)
                    vis_frames.append(np.zeros((TARGET_IMAGE_SIZE, TARGET_IMAGE_SIZE, 3), dtype=np.uint8))

            if len(vis_frames) > 0:
                debug_view = cv2.resize(np.hstack(vis_frames), (224*3, 224))
                cv2.imshow("SmolVLA View", debug_view)
                if cv2.waitKey(1) & 0xFF == ord('q'): break

            # Êé®ÁêÜ
            try:
                with torch.no_grad():
                    raw_action = policy.select_action(batch).squeeze(0)
                    if raw_action.shape[0] != action_mean.shape[0]:
                        limit = min(raw_action.shape[0], action_mean.shape[0])
                        curr_mean, curr_std = action_mean[:limit], action_std[:limit]
                        raw_action = raw_action[:limit]
                    else:
                        curr_mean, curr_std = action_mean, action_std

                    real_action = raw_action * curr_std + curr_mean
                    output_action = real_action.float().cpu().numpy()
            except Exception as e:
                print(f"‚ùå Êé®ÁêÜÂ¥©Ê∫É: {e}")
                break

            target_l = output_action[0:6]
            l_grip_raw = output_action[6]
            target_r = output_action[13:19]
            r_grip_raw = output_action[19]

            l_cmd_smooth = l_cmd_smooth * (1 - SMOOTH_FACTOR) + target_l * SMOOTH_FACTOR
            r_cmd_smooth = r_cmd_smooth * (1 - SMOOTH_FACTOR) + target_r * SMOOTH_FACTOR
            
            l_grip_pos = int(np.clip(l_grip_raw * 10, 0, 1000))
            r_grip_pos = int(np.clip(r_grip_raw * 10, 0, 1000))

            fps = 1.0 / (time.perf_counter() - loop_start)
            print(f"\rFPS: {fps:.1f} | L: {l_grip_pos:<4} | R: {r_grip_pos:<4}", end="")

            if not DRY_RUN:
                arm_l.rm_movej_canfd(l_cmd_smooth.tolist(), False, 0, 0, 0)
                arm_r.rm_movej_canfd(r_cmd_smooth.tolist(), False, 0, 0, 0)
                
                if abs(l_grip_pos - last_l_grip_cmd) > 20: 
                    arm_l.rm_set_gripper_position(l_grip_pos, False, 1)
                    last_l_grip_cmd = l_grip_pos

                if abs(r_grip_pos - last_r_grip_cmd) > 20:
                    arm_r.rm_set_gripper_position(r_grip_pos, False, 1)
                    last_r_grip_cmd = r_grip_pos
            else:
                time.sleep(0.05)

    except KeyboardInterrupt:
        print("\nüëã ÂÅúÊ≠¢ËøêË°å")
    finally:
        arm_l.rm_delete_robot_arm()
        arm_r.rm_delete_robot_arm()
        for cap in caps.values(): cap.disconnect()
        cv2.destroyAllWindows()

if __name__ == "__main__":
    main()