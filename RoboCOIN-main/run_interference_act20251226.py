# import torch
# import time
# import numpy as np
# import os
# from pathlib import Path
# import cv2

# # åŸºç¡€ç»„ä»¶
# from lerobot.policies.act.modeling_act import ACTPolicy
# from lerobot.cameras.realsense.camera_realsense import RealSenseCamera, RealSenseCameraConfig
# from src.lerobot.robots.realman.realman import Realman
# from src.lerobot.robots.realman.configuration_realman import RealmanConfig
# from Robotic_Arm.rm_robot_interface import *

# # ================= é…ç½®åŒº =================
# MODEL_PATH = "/home/robot/lerobot-main/outputs/train/checkpoints/last/pretrained_model"
# LEFT_IP, RIGHT_IP = "169.254.128.18", "169.254.128.19"
# # ç›¸æœºåºåˆ—å·
# CAMERAS = {
#     "image_top": "346522073032",
#     "image_left_wrist": "243722073715",
#     "image_right_wrist": "346522074543",
# }
# # ==========================================


# def main():
#     os.environ["CUDA_VISIBLE_DEVICES"] = "" 
    
#     # 1. åŠ è½½æ¨¡å‹
#     print(f"ğŸš€ æ­£åœ¨åŠ è½½åŸç”Ÿæ¨¡å‹: {MODEL_PATH}")
#     policy = ACTPolicy.from_pretrained(MODEL_PATH)
#     policy.eval()

#     # 2. æŒ‰ç…§å®˜æ–¹ Demo å®ä¾‹åŒ–å’Œè¿æ¥
#     arm_l = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
#     arm_r = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
    
#     handle_l = arm_l.rm_create_robot_arm(LEFT_IP, 8080)
#     handle_r = arm_r.rm_create_robot_arm(RIGHT_IP, 8080)
    
#     if handle_l.id == -1 or handle_r.id == -1:
#         print("âŒ æœºæ¢°è‡‚è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ IP")
#         return
    
#     print(f"âœ… è¿æ¥æˆåŠŸ! ID: Left({handle_l.id}), Right({handle_r.id})")

#     # å¼ºåˆ¶åˆ‡æ¢åˆ°è½¨è¿¹æ¨¡å¼ (Mode 0) ç¡®ä¿ movej ç”Ÿæ•ˆ
#     arm_l.rm_set_arm_run_mode(0)
#     arm_r.rm_set_arm_run_mode(0)

#     # 3. ç›¸æœºåˆå§‹åŒ–
#     caps = {n: cv2.VideoCapture(i) for n, i in CAMERAS.items()}

#     try:
#         print("ğŸ å¼€å§‹åŒæ­¥æ¨ç† (25Hz)...")
#         while True:
#             start_time = time.perf_counter()
            
#             # --- 4. è·å–è§‚æµ‹ (åŸç”Ÿ API è°ƒç”¨) ---
#             # ç›´æ¥è·å–è§’åº¦
#             _, joints_l = arm_l.rm_get_joint_degree()
#             _, joints_r = arm_r.rm_get_joint_degree()
            
#             # è·å–å¤¹çˆªä½ç½®
#             _, grip_l_info = arm_l.rm_get_rm_plus_state_info()
#             _, grip_r_info = arm_r.rm_get_rm_plus_state_info()
#             pos_l = grip_l_info.get('pos', [0])[0]
#             pos_r = grip_r_info.get('pos', [0])[0]
            
#             # æ„é€  26 ç»´çŠ¶æ€ (13+13 ç»“æ„)
#             s_l_7 = np.array(joints_l + [pos_l])
#             s_r_7 = np.array(joints_r + [pos_r])
#             p_zeros = torch.zeros(6)
            
#             full_state = torch.cat([
#                 torch.from_numpy(s_l_7).float(), p_zeros,
#                 torch.from_numpy(s_r_7).float(), p_zeros
#             ]).unsqueeze(0)
            
#             # æ„é€ å›¾åƒè¾“å…¥
#             batch = {"observation.state": full_state}
#             for name, cap in caps.items():
#                 ret, frame = cap.read()
#                 if ret:
#                     img = torch.from_numpy(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).float()
#                     batch[f"observation.images.{name}"] = img.permute(2, 0, 1).unsqueeze(0) / 255.0

#             # --- 5. æ¨ç† ---
#             with torch.no_grad():
#                 action = policy.select_action(batch).squeeze(0).cpu().numpy()

#             # --- 6. åŒè‡‚åŒæ­¥å¹¶è¡Œä¸‹å‘ ---
#             l_joints_cmd = action[0:6].tolist()
#             l_gripper_cmd = int(np.clip(action[6], 0, 1000))
            
#             r_joints_cmd = action[13:19].tolist()
#             r_gripper_cmd = int(np.clip(action[19], 0, 1000))

#             # ğŸš€ å…³é”®ï¼šåŒæ­¥éé˜»å¡ä¸‹å‘ (block=0)
#             # v=20 é€Ÿåº¦, r=0 è§’åº¦
#             arm_l.rm_movej(l_joints_cmd, 20, 0, 1, 0)
#             arm_r.rm_movej(r_joints_cmd, 20, 0, 1, 0)
            
#             # å¤¹çˆªä¸‹å‘
#             arm_l.rm_set_gripper_position(l_gripper_cmd, False, 1)
#             arm_r.rm_set_gripper_position(r_gripper_cmd, False, 1)

#             print(f"L_J1: {joints_l[0]:.1f}->{l_joints_cmd[0]:.1f} | R_J1: {joints_r[0]:.1f}->{r_joints_cmd[0]:.1f}")

#             # ç»´æŒé¢‘ç‡åœ¨ 25Hz å·¦å³
#             elapsed = time.perf_counter() - start_time
#             time.sleep(max(0, 0.04 - elapsed))

#     except KeyboardInterrupt:
#         print("\nğŸ›‘ åœæ­¢")
#     finally:
#         arm_l.rm_delete_robot_arm() # æŒ‰ç…§ Demo æ¸…ç†
#         arm_r.rm_delete_robot_arm()
#         for cap in caps.values(): cap.release()

# if __name__ == "__main__":
#     main()

import torch
import time
import numpy as np
import os
import cv2
from pathlib import Path

# åŸºç¡€ç»„ä»¶
from lerobot.policies.act.modeling_act import ACTPolicy
from lerobot.cameras.realsense.camera_realsense import RealSenseCamera, RealSenseCameraConfig
# æœºå™¨äºº SDK æ¥å£
from Robotic_Arm.rm_robot_interface import RoboticArm, rm_thread_mode_e

# ================= é…ç½®åŒº =================
# MODEL_PATH = "/home/robot/lerobot-main/outputs/train/checkpoints/last/pretrained_model"
MODEL_PATH = "/home/robot/lerobot-main/outputs/train/last/pretrained_model"
LEFT_IP, RIGHT_IP = "169.254.128.18", "169.254.128.19"

CAMERAS_CONFIG = {
    "image_top": "346522073032",
    "image_left_wrist": "243722073715",
    "image_right_wrist": "346522074543",
}

TARGET_KEYS = {
    "image_top": "observation.images.image_top",
    "image_left_wrist": "observation.images.image_left_wrist",
    "image_right_wrist": "observation.images.image_right_wrist"
}
# ==========================================

def main():
    # 1. è®¾å¤‡ç¯å¢ƒè‡ªåŠ¨é€‚é…
    try:
        if torch.cuda.is_available():
            device = torch.device("cuda")
            # é’ˆå¯¹ RTX 50 ç³»åˆ—çš„æ½œåœ¨é©±åŠ¨é—®é¢˜ï¼Œå¼ºåˆ¶åŒæ­¥æŠ¥é”™
            os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
            print(f"ğŸš€ æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}ï¼Œå°è¯•å¼€å¯æ˜¾å¡åŠ é€Ÿ")
        else:
            raise Exception("No CUDA")
    except:
        device = torch.device("cpu")
        torch.set_num_threads(os.cpu_count())
        print("âš ï¸ æ˜¾å¡é©±åŠ¨ä¸å…¼å®¹æˆ–æ— æ˜¾å¡ï¼Œåˆ‡æ¢è‡³ CPU æè‡´ä¼˜åŒ–æ¨¡å¼")

    # 2. åŠ è½½æ¨¡å‹
    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹è‡³ {device}...")
    policy = ACTPolicy.from_pretrained(MODEL_PATH)
    policy.to(device)
    
    # éªŒè¯æ¨¡å‹åŠ è½½åœ¨CPUè¿˜æ˜¯GPUä¸Šäº†
    param_iterator = iter(policy.parameters())
    p = next(param_iterator)
    print("policy_param_device",p.device)
    
    policy.eval()

    # 3. æœºæ¢°è‡‚åˆå§‹åŒ–
    arm_l = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
    arm_r = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
    
    handle_l = arm_l.rm_create_robot_arm(LEFT_IP, 8080)
    handle_r = arm_r.rm_create_robot_arm(RIGHT_IP, 8080)
    
    if handle_l.id == -1 or handle_r.id == -1:
        print("âŒ æœºæ¢°è‡‚è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ IP")
        return

    # ğŸš€ å…³é”®ï¼šå¼ºåˆ¶æ¿€æ´»æœºæ¢°è‡‚æ§åˆ¶æ¨¡å¼ (é˜²æ­¢è¿æ¥æˆåŠŸä½†æ— æ³•è¿åŠ¨)
    # arm_l.rm_set_arm_run_mode(0) # 0 ä»£è¡¨ç¨‹åºæ§åˆ¶æ¨¡å¼
    # arm_r.rm_set_arm_run_mode(0)
    # print("âœ… æœºæ¢°è‡‚æ§åˆ¶æƒé™å·²æ¿€æ´»")

    # 4. ç›¸æœºåˆå§‹åŒ–
    caps = {}
    for name, sn in CAMERAS_CONFIG.items():
        try:
            # ç»Ÿä¸€ä½¿ç”¨ä¸­ç­‰åˆ†è¾¨ç‡æé«˜æ¨ç†é¢‘ç‡
            config = RealSenseCameraConfig(serial_number_or_name=sn, fps=30, width=640, height=480)
            caps[name] = RealSenseCamera(config)
            caps[name].connect()
            print(f"ğŸ“¸ ç›¸æœº {name} å°±ç»ª")
        except Exception as e:
            print(f"âŒ ç›¸æœº {name} å¤±è´¥: {e}")

    print("\nğŸ å¼€å§‹æ¨ç†å¾ªç¯...")
    last_vis_time = 0
    # åŠ¨ä½œåºåˆ—ç¼“å†²
    action_buf = None
    action_idx = 0

    # æ¯æ¬¡æ¨ç†åæ‰§è¡Œå¤šå°‘æ­¥å†æ›´æ–°ä¸€æ¬¡
    execute_k = 10
    try:
        while True:
            loop_start = time.perf_counter()
            
            # --- 5. è·å–ç¡¬ä»¶çŠ¶æ€ ---
            _, joints_l = arm_l.rm_get_joint_degree() 
            _, joints_r = arm_r.rm_get_joint_degree()
            
            # è·å–å¤¹çˆªä½ç½®
            _, l_info = arm_l.rm_get_rm_plus_state_info()
            _, r_info = arm_r.rm_get_rm_plus_state_info()
            grip_pos_l = l_info.get('pos', [0])[0]
            grip_pos_r = r_info.get('pos', [0])[0]
            # print("l_rm_state_info",l_info)  
            # print("r_rm_state_info",r_info)          
        
            # è·å–æœºæ¢°è‡‚ä½å§¿
            _, robot_l_info = arm_l.rm_get_current_arm_state()
            _, robot_r_info = arm_r.rm_get_current_arm_state()
           
            arm_pos_l = robot_l_info.get('pose', [0.0]*6)
            arm_pos_r = robot_r_info.get('pose', [0.0]*6)
            
            print("arm_pos_r",arm_pos_r)
            # æ„é€ è¾“å…¥ state (ACTé€šå¸¸æ˜¯ 14ç»´æˆ–26ç»´ï¼Œè¿™é‡Œæ ¹æ®ä½ ä¹‹å‰ 26ç»´é€»è¾‘)
            state_data = np.concatenate([
                np.array(joints_l + [grip_pos_l]), # 7
                np.array(arm_pos_l), # 6
                np.array(joints_r + [grip_pos_r]), # 7
                np.array(arm_pos_r) # 6
            ])
            full_state = torch.from_numpy(state_data).float().unsqueeze(0).to(device)
            print("full_state",full_state)
            # --- 6. è·å–å¹¶å¤„ç†å›¾åƒ ---
            batch = {"observation.state": full_state}
            vis_frames = []
            do_vis = (time.time() - last_vis_time > 2.0)

            for name, cap in caps.items():
                # frame_rgb = cap.read()
                # if frame_rgb is not None:
                #     # é¢„å¤„ç†ï¼šResize -> Tensor -> Normalize
                #     small_rgb = cv2.resize(frame_rgb, (320, 240))
                #     img_tensor = torch.from_numpy(small_rgb).float().permute(2, 0, 1).unsqueeze(0) / 255.0
                #     batch[TARGET_KEYS[name]] = img_tensor.to(device)
                    
                #     if do_vis:
                #         vis_frames.append(cv2.cvtColor(small_rgb, cv2.COLOR_RGB2BGR))
                # else:
                #     batch[TARGET_KEYS[name]] = torch.zeros((1, 3, 240, 320)).to(device)
                frame_rgb = cap.read()
                if frame_rgb is not None:
                    # ä¸ resizeï¼Œç›´æ¥ç”¨åŸå§‹ 640Ã—480
                    img = frame_rgb  # shape (480, 640, 3)
                    img_tensor = torch.from_numpy(img).float().permute(2, 0, 1).unsqueeze(0) / 255.0
                    batch[TARGET_KEYS[name]] = img_tensor.to(device)

                    if do_vis:
                        vis_frames.append(cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
                else:
                    batch[TARGET_KEYS[name]] = torch.zeros((1, 3, 480, 640)).to(device)




            if do_vis and len(vis_frames) == 3:
                cv2.imwrite("vis_debug.jpg", np.hstack(vis_frames))
                last_vis_time = time.time()

            # --- 7. æ¨ç† (å¸¦æŠ¥é”™æ•è·) ---
            # try:
            #     with torch.no_grad():
            #         # å¾—åˆ°è¾“å‡ºå¹¶è½¬å› CPU numpy
            #         output_action = policy.select_action(batch).squeeze(0).cpu().numpy()
            # except Exception as e:
            #     print(f"âŒ æ¨ç†å´©æºƒ: {e}")
            #     break
            # å¦‚æœæ²¡æœ‰ç¼“å†²ï¼Œæˆ–è€…å·²ç»æ‰§è¡Œäº† execute_k æ­¥ï¼Œå°±é‡æ–°æ¨ç†ä¸€æ¬¡
            # need_new_plan = (action_buf is None) or (action_idx % execute_k == 0)

            # if need_new_plan:
            #     with torch.no_grad():
            #         out = policy.select_action(batch)

            #     out = out.squeeze(0).cpu().numpy()

            #     # å…¼å®¹ 2 ç§è¾“å‡ºå½¢çŠ¶
            #     # å¦‚æœæ˜¯ 100,26 å°±æ˜¯ä¸€æ®µåºåˆ—
            #     # å¦‚æœæ˜¯ 26 å°±æ˜¯ä¸€å¸§åŠ¨ä½œ
            #     if out.ndim == 2:
            #         action_buf = out
            #         action_idx = 0
            #     else:
            #         action_buf = out[None, :]
            #         action_idx = 0

            # # å–å½“å‰è¦æ‰§è¡Œçš„è¿™ä¸€å¸§åŠ¨ä½œ
            # output_action = action_buf[action_idx]
            # action_idx += 1

            # --------------- æ¨ç†ä¸å–åŠ¨ä½œï¼ˆå…¼å®¹å•æ­¥å’Œå¤šæ­¥ï¼‰ ---------------
            with torch.no_grad():
                out = policy.select_action(batch)

            out = out.squeeze(0).detach().cpu().numpy()

            # out å¯èƒ½æ˜¯ (26,) æˆ– (1,26) æˆ– (T,26)
            if out.ndim == 1:
                action_buf = out[None, :]          # (1,26)
            elif out.ndim == 2:
                action_buf = out                  # (T,26)
            else:
                # æå°‘æ•°æƒ…å†µ (1,T,26)
                action_buf = out.reshape(-1, out.shape[-1])

            # å•æ­¥åŠ¨ä½œï¼Œæ¯æ¬¡åªæ‰§è¡Œ action_buf[0]
            output_action = action_buf[0]
            # ------------------------------------------------------------

            # --- 8. ä¸‹å‘åŠ¨ä½œä¸è°ƒè¯•æ‰“å° ---
            #l_cmd = np.rad2deg(output_action[0:6].tolist())
            #r_cmd = np.rad2deg(output_action[13:19].tolist())
            
            l_cmd = output_action[0:6].tolist()
            r_cmd = output_action[13:19].tolist()
            
            print(f"ğŸ‘‰å³æ‰‹: {r_cmd}")

            l_grip_pos = int(output_action[6])
            r_grip_pos = int(output_action[19])
            print(f"ï¿¥ï¿¥å·¦æ‰‹å¤¹çˆªï¿¥ï¿¥r_grip_pos:{l_grip_pos}")    
            print(f"***å³æ‰‹å¤¹çˆª***r_grip_pos:{r_grip_pos}") 
                
            # ğŸ›‘ æ ¸å¿ƒè°ƒè¯•ï¼šè®¡ç®—åŠ¨ä½œå˜åŒ–é‡
            l_diff = np.abs(np.array(l_cmd) - np.array(joints_l)).mean()
            r_diff = np.abs(np.array(r_cmd) - np.array(joints_r)).mean()

            # æ‰§è¡Œè¿åŠ¨ (v=60, block=0)
            # arm_l.rm_movej(l_cmd[0:6], v=25, r=0, connect=1, block=0)
            # arm_r.rm_movej(r_cmd[0:6], v=25, r=0, connect=1, block=0)
            
            arm_l.rm_movej_canfd(l_cmd[0:6], False,0,0,0)
            arm_r.rm_movej_canfd(r_cmd[0:6], False,0,0,0)           
            
            
            
            l_grip_result = arm_l.rm_set_gripper_position(l_grip_pos, False, 1)
            r_grip_result_= arm_r.rm_set_gripper_position(r_grip_pos, False, 1)
            # print(f"ï¿¥ï¿¥å·¦æ‰‹å¤¹çˆªæ‰§è¡Œç»“æœï¿¥ï¿¥:{l_grip_result}")  
            print(f"***å³æ‰‹å¤¹çˆªæ‰§è¡Œç»“æœ***:{r_grip_result_}") 



            # æ‰“å°ç›‘æ§
            fps = 1.0 / (time.perf_counter() - loop_start)
            print(f"FPS: {fps:4.1f} | L_Diff: {l_diff:6.4f} | R_Diff: {r_diff:6.4f} | æŒ‡ä»¤: {r_cmd[0]:.2f}")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ åœæ­¢è¿è¡Œ")
    finally:
        arm_l.rm_delete_robot_arm()
        arm_r.rm_delete_robot_arm()
        for cap in caps.values():
            cap.disconnect()

if __name__ == "__main__":
    main()