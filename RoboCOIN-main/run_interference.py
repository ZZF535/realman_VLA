# import torch
# import time
# import numpy as np
# import os
# from pathlib import Path
# import cv2

# # åŸºç¡€ç»„ä»¶
# from lerobot.policies.act.modeling_act import ACTPolicy
# from lerobot.cameras.realsense.camera_realsense import RealSenseCamera, RealSenseCameraConfig
# from src.lerobot.robots.realman.realman import Realman
# from src.lerobot.robots.realman.configuration_realman import RealmanConfig
# from Robotic_Arm.rm_robot_interface import *

# # ================= é…ç½®åŒº =================
# MODEL_PATH = "/home/robot/lerobot-main/outputs/train/checkpoints/last/pretrained_model"
# LEFT_IP, RIGHT_IP = "169.254.128.18", "169.254.128.19"
# # ç›¸æœºåºåˆ—å·
# CAMERAS = {
#     "image_top": "346522073032",
#     "image_left_wrist": "243722073715",
#     "image_right_wrist": "346522074543",
# }
# # ==========================================


# def main():
#     os.environ["CUDA_VISIBLE_DEVICES"] = "" 
    
#     # 1. åŠ è½½æ¨¡å‹
#     print(f"ğŸš€ æ­£åœ¨åŠ è½½åŸç”Ÿæ¨¡å‹: {MODEL_PATH}")
#     policy = ACTPolicy.from_pretrained(MODEL_PATH)
#     policy.eval()

#     # 2. æŒ‰ç…§å®˜æ–¹ Demo å®ä¾‹åŒ–å’Œè¿æ¥
#     arm_l = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
#     arm_r = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
    
#     handle_l = arm_l.rm_create_robot_arm(LEFT_IP, 8080)
#     handle_r = arm_r.rm_create_robot_arm(RIGHT_IP, 8080)
    
#     if handle_l.id == -1 or handle_r.id == -1:
#         print("âŒ æœºæ¢°è‡‚è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ IP")
#         return
    
#     print(f"âœ… è¿æ¥æˆåŠŸ! ID: Left({handle_l.id}), Right({handle_r.id})")

#     # å¼ºåˆ¶åˆ‡æ¢åˆ°è½¨è¿¹æ¨¡å¼ (Mode 0) ç¡®ä¿ movej ç”Ÿæ•ˆ
#     arm_l.rm_set_arm_run_mode(0)
#     arm_r.rm_set_arm_run_mode(0)

#     # 3. ç›¸æœºåˆå§‹åŒ–
#     caps = {n: cv2.VideoCapture(i) for n, i in CAMERAS.items()}

#     try:
#         print("ğŸ å¼€å§‹åŒæ­¥æ¨ç† (25Hz)...")
#         while True:
#             start_time = time.perf_counter()
            
#             # --- 4. è·å–è§‚æµ‹ (åŸç”Ÿ API è°ƒç”¨) ---
#             # ç›´æ¥è·å–è§’åº¦
#             _, joints_l = arm_l.rm_get_joint_degree()
#             _, joints_r = arm_r.rm_get_joint_degree()
            
#             # è·å–å¤¹çˆªä½ç½®
#             _, grip_l_info = arm_l.rm_get_rm_plus_state_info()
#             _, grip_r_info = arm_r.rm_get_rm_plus_state_info()
#             pos_l = grip_l_info.get('pos', [0])[0]
#             pos_r = grip_r_info.get('pos', [0])[0]
            
#             # æ„é€  26 ç»´çŠ¶æ€ (13+13 ç»“æ„)
#             s_l_7 = np.array(joints_l + [pos_l])
#             s_r_7 = np.array(joints_r + [pos_r])
#             p_zeros = torch.zeros(6)
            
#             full_state = torch.cat([
#                 torch.from_numpy(s_l_7).float(), p_zeros,
#                 torch.from_numpy(s_r_7).float(), p_zeros
#             ]).unsqueeze(0)
            
#             # æ„é€ å›¾åƒè¾“å…¥
#             batch = {"observation.state": full_state}
#             for name, cap in caps.items():
#                 ret, frame = cap.read()
#                 if ret:
#                     img = torch.from_numpy(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).float()
#                     batch[f"observation.images.{name}"] = img.permute(2, 0, 1).unsqueeze(0) / 255.0

#             # --- 5. æ¨ç† ---
#             with torch.no_grad():
#                 action = policy.select_action(batch).squeeze(0).cpu().numpy()

#             # --- 6. åŒè‡‚åŒæ­¥å¹¶è¡Œä¸‹å‘ ---
#             l_joints_cmd = action[0:6].tolist()
#             l_gripper_cmd = int(np.clip(action[6], 0, 1000))
            
#             r_joints_cmd = action[13:19].tolist()
#             r_gripper_cmd = int(np.clip(action[19], 0, 1000))

#             # ğŸš€ å…³é”®ï¼šåŒæ­¥éé˜»å¡ä¸‹å‘ (block=0)
#             # v=20 é€Ÿåº¦, r=0 è§’åº¦
#             arm_l.rm_movej(l_joints_cmd, 20, 0, 1, 0)
#             arm_r.rm_movej(r_joints_cmd, 20, 0, 1, 0)
            
#             # å¤¹çˆªä¸‹å‘
#             arm_l.rm_set_gripper_position(l_gripper_cmd, False, 1)
#             arm_r.rm_set_gripper_position(r_gripper_cmd, False, 1)

#             print(f"L_J1: {joints_l[0]:.1f}->{l_joints_cmd[0]:.1f} | R_J1: {joints_r[0]:.1f}->{r_joints_cmd[0]:.1f}")

#             # ç»´æŒé¢‘ç‡åœ¨ 25Hz å·¦å³
#             elapsed = time.perf_counter() - start_time
#             time.sleep(max(0, 0.04 - elapsed))

#     except KeyboardInterrupt:
#         print("\nğŸ›‘ åœæ­¢")
#     finally:
#         arm_l.rm_delete_robot_arm() # æŒ‰ç…§ Demo æ¸…ç†
#         arm_r.rm_delete_robot_arm()
#         for cap in caps.values(): cap.release()

# if __name__ == "__main__":
#     main()

# import torch
# import time
# import numpy as np
# import os
# import cv2
# from pathlib import Path

# # åŸºç¡€ç»„ä»¶
# from lerobot.policies.act.modeling_act import ACTPolicy
# from lerobot.cameras.realsense.camera_realsense import RealSenseCamera, RealSenseCameraConfig
# # æœºå™¨äºº SDK æ¥å£
# from Robotic_Arm.rm_robot_interface import RoboticArm, rm_thread_mode_e

# # ================= é…ç½®åŒº =================
# # MODEL_PATH = "/home/robot/lerobot-main/outputs/train/checkpoints/last/pretrained_model"
# MODEL_PATH = "/home/robot/lerobot-main/outputs/train/last/pretrained_model"
# LEFT_IP, RIGHT_IP = "169.254.128.18", "169.254.128.19"

# CAMERAS_CONFIG = {
#     "image_top": "346522073032",
#     "image_left_wrist": "243722073715",
#     "image_right_wrist": "346522074543",
# }

# TARGET_KEYS = {
#     "image_top": "observation.images.image_top",
#     "image_left_wrist": "observation.images.image_left_wrist",
#     "image_right_wrist": "observation.images.image_right_wrist"
# }
# # ==========================================

# def main():
#     # 1. è®¾å¤‡ç¯å¢ƒè‡ªåŠ¨é€‚é…
#     try:
#         if torch.cuda.is_available():
#             device = torch.device("cuda")
#             # é’ˆå¯¹ RTX 50 ç³»åˆ—çš„æ½œåœ¨é©±åŠ¨é—®é¢˜ï¼Œå¼ºåˆ¶åŒæ­¥æŠ¥é”™
#             os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
#             print(f"ğŸš€ æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}ï¼Œå°è¯•å¼€å¯æ˜¾å¡åŠ é€Ÿ")
#         else:
#             raise Exception("No CUDA")
#     except:
#         device = torch.device("cpu")
#         torch.set_num_threads(os.cpu_count())
#         print("âš ï¸ æ˜¾å¡é©±åŠ¨ä¸å…¼å®¹æˆ–æ— æ˜¾å¡ï¼Œåˆ‡æ¢è‡³ CPU æè‡´ä¼˜åŒ–æ¨¡å¼")

#     # 2. åŠ è½½æ¨¡å‹
#     print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹è‡³ {device}...")
#     policy = ACTPolicy.from_pretrained(MODEL_PATH)
#     policy.to(device)
    
#     # éªŒè¯æ¨¡å‹åŠ è½½åœ¨CPUè¿˜æ˜¯GPUä¸Šäº†
#     param_iterator = iter(policy.parameters())
#     p = next(param_iterator)
#     print("policy_param_device",p.device)
    
#     policy.eval()

#     # 3. æœºæ¢°è‡‚åˆå§‹åŒ–
#     arm_l = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
#     arm_r = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
    
#     handle_l = arm_l.rm_create_robot_arm(LEFT_IP, 8080)
#     handle_r = arm_r.rm_create_robot_arm(RIGHT_IP, 8080)
    
#     if handle_l.id == -1 or handle_r.id == -1:
#         print("âŒ æœºæ¢°è‡‚è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ IP")
#         return

#     # ğŸš€ å…³é”®ï¼šå¼ºåˆ¶æ¿€æ´»æœºæ¢°è‡‚æ§åˆ¶æ¨¡å¼ (é˜²æ­¢è¿æ¥æˆåŠŸä½†æ— æ³•è¿åŠ¨)
#     # arm_l.rm_set_arm_run_mode(0) # 0 ä»£è¡¨ç¨‹åºæ§åˆ¶æ¨¡å¼
#     # arm_r.rm_set_arm_run_mode(0)
#     # print("âœ… æœºæ¢°è‡‚æ§åˆ¶æƒé™å·²æ¿€æ´»")

#     # 4. ç›¸æœºåˆå§‹åŒ–
#     caps = {}
#     for name, sn in CAMERAS_CONFIG.items():
#         try:
#             # ç»Ÿä¸€ä½¿ç”¨ä¸­ç­‰åˆ†è¾¨ç‡æé«˜æ¨ç†é¢‘ç‡
#             config = RealSenseCameraConfig(serial_number_or_name=sn, fps=30, width=640, height=480)
#             caps[name] = RealSenseCamera(config)
#             caps[name].connect()
#             print(f"ğŸ“¸ ç›¸æœº {name} å°±ç»ª")
#         except Exception as e:
#             print(f"âŒ ç›¸æœº {name} å¤±è´¥: {e}")

#     print("\nğŸ å¼€å§‹æ¨ç†å¾ªç¯...")
#     last_vis_time = 0

#     try:
#         while True:
#             loop_start = time.perf_counter()
            
#             # --- 5. è·å–ç¡¬ä»¶çŠ¶æ€ ---
#             _, joints_l = arm_l.rm_get_joint_degree() 
#             _, joints_r = arm_r.rm_get_joint_degree()
            
#             # æ„é€ è¾“å…¥ state (ACTé€šå¸¸æ˜¯ 14ç»´æˆ–26ç»´ï¼Œè¿™é‡Œæ ¹æ®ä½ ä¹‹å‰ 26ç»´é€»è¾‘)
#             # å‡è®¾åªå…³å¿ƒ 6è½´+å¤¹çˆªï¼Œå…¶ä½™è¡¥0
#             state_data = np.concatenate([
#                 np.array(joints_l + [0]), # 7
#                 np.zeros(6),              # 6
#                 np.array(joints_r + [0]), # 7
#                 np.zeros(6)               # 6
#             ])
#             full_state = torch.from_numpy(state_data).float().unsqueeze(0).to(device)
            
#             # --- 6. è·å–å¹¶å¤„ç†å›¾åƒ ---
#             batch = {"observation.state": full_state}
#             vis_frames = []
#             do_vis = (time.time() - last_vis_time > 2.0)

#             for name, cap in caps.items():
#                 # frame_rgb = cap.read()
#                 # if frame_rgb is not None:
#                 #     # é¢„å¤„ç†ï¼šResize -> Tensor -> Normalize
#                 #     small_rgb = cv2.resize(frame_rgb, (320, 240))
#                 #     img_tensor = torch.from_numpy(small_rgb).float().permute(2, 0, 1).unsqueeze(0) / 255.0
#                 #     batch[TARGET_KEYS[name]] = img_tensor.to(device)
                    
#                 #     if do_vis:
#                 #         vis_frames.append(cv2.cvtColor(small_rgb, cv2.COLOR_RGB2BGR))
#                 # else:
#                 #     batch[TARGET_KEYS[name]] = torch.zeros((1, 3, 240, 320)).to(device)
#                 frame_rgb = cap.read()
#                 if frame_rgb is not None:
#                     # ä¸ resizeï¼Œç›´æ¥ç”¨åŸå§‹ 640Ã—480
#                     img = frame_rgb  # shape (480, 640, 3)
#                     img_tensor = torch.from_numpy(img).float().permute(2, 0, 1).unsqueeze(0) / 255.0
#                     batch[TARGET_KEYS[name]] = img_tensor.to(device)

#                     if do_vis:
#                         vis_frames.append(cv2.cvtColor(img, cv2.COLOR_RGB2BGR))
#                 else:
#                     batch[TARGET_KEYS[name]] = torch.zeros((1, 3, 480, 640)).to(device)




#             if do_vis and len(vis_frames) == 3:
#                 cv2.imwrite("vis_debug.jpg", np.hstack(vis_frames))
#                 last_vis_time = time.time()

#             # --- 7. æ¨ç† (å¸¦æŠ¥é”™æ•è·) ---
#             try:
#                 with torch.no_grad():
#                     # å¾—åˆ°è¾“å‡ºå¹¶è½¬å› CPU numpy
#                     output_action = policy.select_action(batch).squeeze(0).cpu().numpy()
#             except Exception as e:
#                 print(f"âŒ æ¨ç†å´©æºƒ: {e}")
#                 break

#             # --- 8. ä¸‹å‘åŠ¨ä½œä¸è°ƒè¯•æ‰“å° ---
#             #l_cmd = np.rad2deg(output_action[0:6].tolist())
#             #r_cmd = np.rad2deg(output_action[13:19].tolist())
            
#             l_cmd = output_action[0:6].tolist()
#             r_cmd = output_action[13:19].tolist()
            
#             print(f"ğŸ‘‰å³æ‰‹: {r_cmd}")

#             l_grip_pos = int(output_action[6])
#             r_grip_pos = int(output_action[19])
#             # print(f"ï¿¥ï¿¥å·¦æ‰‹å¤¹çˆªï¿¥ï¿¥r_grip_pos:{l_grip_pos}")    
#             print(f"***å³æ‰‹å¤¹çˆª***r_grip_pos:{r_grip_pos}") 
                
#             # ğŸ›‘ æ ¸å¿ƒè°ƒè¯•ï¼šè®¡ç®—åŠ¨ä½œå˜åŒ–é‡
#             l_diff = np.abs(np.array(l_cmd) - np.array(joints_l)).mean()
#             r_diff = np.abs(np.array(r_cmd) - np.array(joints_r)).mean()

#             # æ‰§è¡Œè¿åŠ¨ (v=60, block=0)
#             # arm_l.rm_movej(l_cmd[0:6], v=25, r=0, connect=1, block=0)
#             # arm_r.rm_movej(r_cmd[0:6], v=25, r=0, connect=1, block=0)
            
#             arm_l.rm_movej_canfd(l_cmd[0:6], False,0,0,0)
#             arm_r.rm_movej_canfd(r_cmd[0:6], False,0,0,0)           
            
            
            
#             l_grip_result = arm_l.rm_set_gripper_position(l_grip_pos, False, 1)
#             r_grip_result_= arm_r.rm_set_gripper_position(r_grip_pos, False, 1)
#             # print(f"ï¿¥ï¿¥å·¦æ‰‹å¤¹çˆªæ‰§è¡Œç»“æœï¿¥ï¿¥:{l_grip_result}")  
#             print(f"***å³æ‰‹å¤¹çˆªæ‰§è¡Œç»“æœ***:{r_grip_result_}") 



#             # æ‰“å°ç›‘æ§
#             fps = 1.0 / (time.perf_counter() - loop_start)
#             print(f"FPS: {fps:4.1f} | L_Diff: {l_diff:6.4f} | R_Diff: {r_diff:6.4f} | æŒ‡ä»¤: {r_cmd[0]:.2f}")

#     except KeyboardInterrupt:
#         print("\nğŸ‘‹ åœæ­¢è¿è¡Œ")
#     finally:
#         arm_l.rm_delete_robot_arm()
#         arm_r.rm_delete_robot_arm()
#         for cap in caps.values():
#             cap.disconnect()

# if __name__ == "__main__":
#     main()

import torch
import time
import numpy as np
import os
import cv2
from pathlib import Path

# åŸºç¡€ç»„ä»¶
from lerobot.policies.act.modeling_act import ACTPolicy
from lerobot.cameras.realsense.camera_realsense import RealSenseCamera, RealSenseCameraConfig
# æœºå™¨äºº SDK æ¥å£
from Robotic_Arm.rm_robot_interface import RoboticArm, rm_thread_mode_e

# ================= é…ç½®åŒº =================
# MODEL_PATH = "/home/robot/lerobot-main/outputs/train/checkpoints/last/pretrained_model"
MODEL_PATH = "/home/robot/lerobot-main/outputs/train/005000/pretrained_model"
LEFT_IP, RIGHT_IP = "169.254.128.18", "169.254.128.19"

CAMERAS_CONFIG = {
    "image_top": "346522073032",
    "image_left_wrist": "243722073715",
    "image_right_wrist": "346522074543",
}

TARGET_KEYS = {
    "image_top": "observation.images.image_top",
    "image_left_wrist": "observation.images.image_left_wrist",
    "image_right_wrist": "observation.images.image_right_wrist"
}
# ==========================================

def main():
    # 1. è®¾å¤‡ç¯å¢ƒè‡ªåŠ¨é€‚é…
    try:
        if torch.cuda.is_available():
            device = torch.device("cuda")
            os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
            print(f"ğŸš€ æ£€æµ‹åˆ° GPU: {torch.cuda.get_device_name(0)}ï¼Œå°è¯•å¼€å¯æ˜¾å¡åŠ é€Ÿ")
        else:
            raise Exception("No CUDA")
    except:
        device = torch.device("cpu")
        torch.set_num_threads(os.cpu_count())
        print("âš ï¸ æ˜¾å¡é©±åŠ¨ä¸å…¼å®¹æˆ–æ— æ˜¾å¡ï¼Œåˆ‡æ¢è‡³ CPU æè‡´ä¼˜åŒ–æ¨¡å¼")

    # 2. åŠ è½½æ¨¡å‹
    print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹è‡³ {device}...")
    try:
        policy = ACTPolicy.from_pretrained(MODEL_PATH)
        policy.to(device)
        policy.eval()
        
        # ğŸš¨ã€å…³é”®ã€‘å¯ç”¨ Temporal Ensemble å¹³æ»‘åŠ¨ä½œï¼Œå‡å°‘æŠ–åŠ¨
        # ACT è®ºæ–‡æ¨èç³»æ•° 0.01ï¼Œå€¼è¶Šå°è¶Šå¹³æ»‘
        if policy.config.temporal_ensemble_coeff is None:
            print("âš ï¸ æ£€æµ‹åˆ°æœªå¯ç”¨ Temporal Ensembleï¼Œæ­£åœ¨æ‰‹åŠ¨å¯ç”¨...")
            from lerobot.policies.act.modeling_act import ACTTemporalEnsembler
            policy.config.temporal_ensemble_coeff = 0.01
            policy.temporal_ensembler = ACTTemporalEnsembler(
                temporal_ensemble_coeff=0.01, 
                chunk_size=policy.config.chunk_size
            )
            print("âœ… Temporal Ensemble å·²å¯ç”¨ (coeff=0.01)")
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # 3. æœºæ¢°è‡‚åˆå§‹åŒ–
    arm_l = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
    arm_r = RoboticArm(rm_thread_mode_e.RM_TRIPLE_MODE_E)
    
    handle_l = arm_l.rm_create_robot_arm(LEFT_IP, 8080)
    handle_r = arm_r.rm_create_robot_arm(RIGHT_IP, 8080)
    
    if handle_l.id == -1 or handle_r.id == -1:
        print("âŒ æœºæ¢°è‡‚è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ IP")
        return

    # 4. ç›¸æœºåˆå§‹åŒ–
    caps = {}
    for name, sn in CAMERAS_CONFIG.items():
        try:
            # ç»Ÿä¸€ä½¿ç”¨ä¸­ç­‰åˆ†è¾¨ç‡æé«˜æ¨ç†é¢‘ç‡
            config = RealSenseCameraConfig(serial_number_or_name=sn, fps=30, width=640, height=480)
            caps[name] = RealSenseCamera(config)
            caps[name].connect()
            print(f"ğŸ“¸ ç›¸æœº {name} å°±ç»ª")
        except Exception as e:
            print(f"âŒ ç›¸æœº {name} å¤±è´¥: {e}")

    print("\nğŸ å¼€å§‹æ¨ç†å¾ªç¯...")
    last_vis_time = 0
    
    # ğŸš¨ã€å…³é”®ã€‘æ¯æ¬¡å¼€å§‹æ–°ä»»åŠ¡å‰å¿…é¡»é‡ç½®ç­–ç•¥çŠ¶æ€ï¼
    # è¿™ä¼šæ¸…ç©º action queue æˆ–é‡ç½® temporal ensembler
    policy.reset()
    print("âœ… ç­–ç•¥çŠ¶æ€å·²é‡ç½®")

    try:
        while True:
            loop_start = time.perf_counter()
            
            # --- 5. è·å–ç¡¬ä»¶çŠ¶æ€ ---
            # è·å–è§’åº¦æ•°æ®
            _, joints_l_deg = arm_l.rm_get_joint_degree() 
            _, joints_r_deg = arm_r.rm_get_joint_degree()
            
            # ğŸš¨ã€æ ¸å¿ƒä¿®å¤ã€‘ï¼šæ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„æ˜¯è§’åº¦ï¼Œæ‰€ä»¥ä¸éœ€è¦è½¬å¼§åº¦ï¼
            # ç›´æ¥ä½¿ç”¨è§’åº¦æ„é€ è¾“å…¥ state
            state_data = np.concatenate([
                np.array(joints_l_deg + [0]),  # 7 (è§’åº¦)
                np.zeros(6),                    # 6 (ä½å§¿å ä½)
                np.array(joints_r_deg + [0]),  # 7 (è§’åº¦)
                np.zeros(6)                     # 6 (ä½å§¿å ä½)
            ])
            full_state = torch.from_numpy(state_data).float().unsqueeze(0).to(device)
            
            # --- 6. è·å–å¹¶å¤„ç†å›¾åƒ ---
            batch = {"observation.state": full_state}
            vis_frames = []
            do_vis = (time.time() - last_vis_time > 2.0)

            for name, cap in caps.items():
                # ğŸš¨ã€æ ¸å¿ƒä¿®å¤ã€‘ï¼šRealSenseCamera.read() é»˜è®¤è¿”å› RGBï¼Œä¸æ˜¯ BGRï¼
                # æŸ¥çœ‹ camera_realsense.py ç¬¬ 274 è¡Œï¼šrs.format.rgb8
                frame_rgb = cap.read()
                
                if frame_rgb is not None:
                    # ç›¸æœºå·²ç»æ˜¯ RGBï¼Œæ¨¡å‹ä¹Ÿæ˜¯ RGB è®­ç»ƒçš„ï¼Œç›´æ¥ç”¨ï¼
                    # ä¸éœ€è¦ä»»ä½•é¢œè‰²è½¬æ¢
                    
                    # è½¬ Tensor (å½’ä¸€åŒ–åˆ° 0-1)
                    img_tensor = torch.from_numpy(frame_rgb).float().permute(2, 0, 1).unsqueeze(0) / 255.0
                    batch[TARGET_KEYS[name]] = img_tensor.to(device)

                    if do_vis:
                        # å¯è§†åŒ–å­˜å‚¨æ—¶è½¬ BGR (å› ä¸º cv2.imwrite éœ€è¦ BGR)
                        vis_frames.append(cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR))
                else:
                    # ä¸¢å¸§è¡¥é»‘å›¾ (æ³¨æ„å°ºå¯¸ 480, 640)
                    batch[TARGET_KEYS[name]] = torch.zeros((1, 3, 480, 640)).to(device)

            if do_vis and len(vis_frames) == 3:
                cv2.imwrite("vis_debug.jpg", np.hstack(vis_frames))
                last_vis_time = time.time()

            # --- 7. æ¨ç† ---
            try:
                with torch.no_grad():
                    output_action = policy.select_action(batch).squeeze(0).cpu().numpy()
            except Exception as e:
                print(f"âŒ æ¨ç†å´©æºƒ: {e}")
                break

            # --- 8. ä¸‹å‘åŠ¨ä½œ ---
            # ğŸš¨ã€æ ¸å¿ƒé€»è¾‘ã€‘ï¼šè¾“å‡ºç›´æ¥ç”¨è§’åº¦ (æ ¹æ®æ—¥å¿—è§‚æµ‹)
            l_cmd = output_action[0:6].tolist()
            r_cmd = output_action[13:19].tolist()
            
            l_grip_pos = int(output_action[6])
            r_grip_pos = int(output_action[19])
            
            # --- 9. è®¡ç®—è¯¯å·® ---
            # æ¨¡å‹è¾“å‡ºå’Œå½“å‰å…³èŠ‚è§’åº¦çš„å·®å€¼ï¼ˆå•ä½ï¼šè§’åº¦ï¼‰
            l_diff = np.abs(np.array(l_cmd) - np.array(joints_l_deg)).mean()
            r_diff = np.abs(np.array(r_cmd) - np.array(joints_r_deg)).mean()

            print(f"ğŸ‘‰å³æ‰‹æŒ‡ä»¤: {r_cmd[0]:.2f} | å¤¹çˆª: {r_grip_pos}")

            # æ‰§è¡Œ (å‘é€è§’åº¦)
            arm_l.rm_movej_canfd(l_cmd, False, 0, 0, 0)
            arm_r.rm_movej_canfd(r_cmd, False, 0, 0, 0)
            
            arm_l.rm_set_gripper_position(l_grip_pos, False, 1)
            arm_r.rm_set_gripper_position(r_grip_pos, False, 1)

            # æ‰“å°ç›‘æ§
            elapsed = time.perf_counter() - loop_start
            fps = 1.0 / elapsed if elapsed > 0 else 0
            print(f"FPS: {fps:4.1f} | L_Diff: {l_diff:6.2f} | R_Diff: {r_diff:6.2f} | æŒ‡ä»¤: {r_cmd[0]:.2f}")
            
            # ğŸš¨ã€å…³é”®ã€‘æ§åˆ¶æ‰§è¡Œé¢‘ç‡ï¼Œé˜²æ­¢åŠ¨ä½œè¿‡å¿«å¯¼è‡´æŠ–åŠ¨
            # ç›®æ ‡é¢‘ç‡ 30Hzï¼ˆä¸æ•°æ®é‡‡é›†é¢‘ç‡ä¸€è‡´ï¼‰
            TARGET_DT = 1.0 / 30.0
            sleep_time = TARGET_DT - elapsed
            if sleep_time > 0:
                time.sleep(sleep_time)

    except KeyboardInterrupt:
        print("\nğŸ‘‹ åœæ­¢è¿è¡Œ")
    finally:
        arm_l.rm_delete_robot_arm()
        arm_r.rm_delete_robot_arm()
        for cap in caps.values():
            cap.disconnect()

if __name__ == "__main__":
    main()